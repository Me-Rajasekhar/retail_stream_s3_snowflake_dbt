# Retail Streaming Medallion Architecture

This project demonstrates a **real-time streaming data pipeline** for a retail store using the **Medallion Architecture (Raw â†’ Bronze â†’ Silver â†’ Gold)**. The pipeline simulates streaming purchase events, stores them in AWS S3, performs cleaning with AWS Glue, stages data in Snowflake via Snowpipe, transforms it using dbt, and prepares enriched data for BI dashboards.

---

## ğŸ§© Architecture Overview

```
Python Simulator â†’ S3 (Raw Layer)
â†’ AWS Glue (Cleaning) â†’ S3 (Bronze Layer)
â†’ Snowpipe â†’ Snowflake (Staging)
â†’ dbt (Transformations) â†’ Snowflake (Silver + Gold)
â†’ BI Tools (Tableau/Power BI/QuickSight)
```

### Layers
- **Raw**: Unprocessed JSON events generated by the simulator.
- **Bronze**: Cleaned and normalized data stored as Parquet via Glue.
- **Silver**: Transformed, deduplicated, and enriched data in Snowflake via dbt.
- **Gold**: Aggregated metrics ready for analytics and BI reporting.

---

## ğŸ“‚ Project Structure

```
retail-streaming-medallion/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ generate_events.py          # Simulates real-time purchase events and pushes to S3
â”‚   â””â”€â”€ glue/glue_job_cleaner.py    # AWS Glue job script to clean and partition data
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bronze.sql              # Raw to Bronze model
â”‚   â”‚   â”œâ”€â”€ silver.sql              # Bronze to Silver model
â”‚   â”‚   â”œâ”€â”€ gold.sql                # Silver to Gold aggregation model
â”‚   â”‚   â”œâ”€â”€ dim_products.sql        # Product dimension for relationship testing
â”‚   â”‚   â””â”€â”€ schema.yml              # Tests: uniqueness, not_null, relationships, accepted_values
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ test_total_amount_consistency.sql  # Custom test for price Ã— quantity consistency
â”‚   â”‚   â””â”€â”€ schema.yml              # Register custom test
â”‚   â”œâ”€â”€ TESTS_README.md             # dbt test documentation
â”‚   â”œâ”€â”€ dbt_project.yml             # dbt configuration
â”‚   â””â”€â”€ profiles.yml                # dbt connection profile example
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â”œâ”€â”€ main.tf                 # S3, IAM, Glue, SNS, SQS resources
â”‚   â”‚   â”œâ”€â”€ variables.tf            # Configurable parameters (bucket name, region, Snowflake ARN)
â”‚   â”‚   â”œâ”€â”€ outputs.tf              # Terraform outputs (ARNs, URLs)
â”‚   â”‚   â””â”€â”€ README.md               # Terraform setup instructions
â”‚
â””â”€â”€ README.md                       # (this file)
```

---

## âš™ï¸ Setup & Execution

### 1. Create & Configure AWS Infrastructure
Use Terraform to provision AWS resources:
```bash
cd infra/terraform
terraform init
terraform apply
```
Edit `variables.tf` to specify:
- A unique S3 bucket name
- Your Snowflake AWS principal ARN

After Terraform finishes:
- Upload the Glue script to `s3://<bucket>/glue/scripts/glue_job_cleaner.py`
- Configure the Snowflake **storage integration** using the IAM role ARN output by Terraform
- Connect Snowpipe to the SNS topic/SQS queue for automatic ingestion

### 2. Run Data Simulation
```bash
cd src
python generate_events.py
```
This script generates continuous JSON purchase events and uploads them to the **raw S3 path**.

### 3. Run AWS Glue Job
In AWS Console â†’ Glue â†’ Jobs, trigger the `retail_glue_cleaner` job to:
- Read data from `raw/`
- Clean & validate events
- Write to `bronze/` partitioned by date

### 4. Configure Snowpipe & Stage Data
Use the SQL scripts provided in the repo (in `/infra/snowflake.sql`) to:
- Create external stage linked to S3 bucket
- Define Snowpipe for Bronze data
- Auto-ingest Bronze â†’ Snowflake staging table

### 5. Run dbt Transformations
```bash
cd dbt
dbt deps
dbt run
dbt test
```

#### Added Tests
- **Built-in tests**: `not_null`, `unique`, `relationships`, `accepted_values`
- **Custom test**: Checks `total_amount â‰ˆ price Ã— quantity`

### 6. BI Reporting
The **Gold** layer aggregates daily revenue and transaction metrics. Connect this to:
- Tableau, Power BI, or QuickSight via Snowflake connector.

---

## ğŸ§ª dbt Tests Summary

| Layer | Model | Test Type | Description |
|--------|--------|------------|--------------|
| Bronze | purchases_bronze | not_null, unique | Validate event_id and timestamps |
| Silver | purchases_silver | relationships, accepted_values | Ensure valid product IDs, nonnegative prices |
| Gold | purchases_gold | custom test | Validate total_amount = price Ã— quantity |

Run tests with:
```bash
dbt test
```

---

## â˜ï¸ Terraform Components Summary

| Resource | Purpose |
|-----------|----------|
| `aws_s3_bucket.retail_bucket` | Stores raw and bronze data |
| `aws_iam_role.snowflake_integration_role` | Role assumed by Snowflake for S3 access |
| `aws_glue_job.glue_cleaner` | Cleans and prepares data from raw to bronze |
| `aws_sns_topic.s3_notifications` | Publishes new S3 object events |
| `aws_sqs_queue.snowpipe_queue` | Snowpipe auto-ingest trigger |

---

## ğŸ§° Tech Stack

- **Python 3.9+** â€” Event simulation
- **AWS S3** â€” Object storage for data lake
- **AWS Glue** â€” ETL for data cleaning
- **Snowflake + Snowpipe** â€” Data warehouse & ingestion
- **dbt** â€” Transformations, tests, and documentation
- **Terraform** â€” Infrastructure as Code (IaC)

---

## ğŸ” Security Notes
- S3 encryption (AES-256) and versioning are enabled by default.
- IAM roles are least-privilege (Snowflake restricted to `raw/*` and `bronze/*`).
- SNS/SQS notifications restricted to internal ARNs.

---

## ğŸš€ Next Steps
- Add Glue job triggers (event-based or schedule)
- Extend dbt tests for data freshness and referential integrity
- Integrate CI/CD with GitHub Actions for dbt testing
- Create BI dashboard template (Tableau/Power BI)

---

**Author:** Rajasekhar Anderson  
**Version:** 2.0 â€” Includes dbt granular tests + Terraform IaC for full infra provisioning.

